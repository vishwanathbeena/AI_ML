{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text procssing using various NLP algorithms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science and technology are strongly related things and important things too in life', 'Technology enables humans to simplify life', 'there is more to life than just eating and sleeping', 'life is what you make it to be']\n"
     ]
    }
   ],
   "source": [
    "#constructing text_array for testing purpose\n",
    "text_array = []\n",
    "string_1 = 'science and technology are strongly related things and important things too in life'\n",
    "string_2 = 'Technology enables humans to simplify life'\n",
    "string_3 = 'there is more to life than just eating and sleeping'\n",
    "string_4 = 'life is what you make it to be'\n",
    "text_array.append(string_1)\n",
    "text_array.append(string_2)\n",
    "text_array.append(string_3)\n",
    "text_array.append(string_4)\n",
    "print(text_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragular bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names  ['and', 'are', 'be', 'eating', 'enables', 'humans', 'important', 'in', 'is', 'it', 'just', 'life', 'make', 'more', 'related', 'science', 'simplify', 'sleeping', 'strongly', 'technology', 'than', 'there', 'things', 'to', 'too', 'what', 'you']\n",
      "==================================================\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(4, 27)\n",
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 22)\t2\n",
      "  (0, 24)\t1\n",
      "[[2 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 2 0 1 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(text_array)\n",
    "print(\"some feature names \", count_vect.get_feature_names())\n",
    "print('='*50)\n",
    "final_counts = count_vect.transform(text_array)\n",
    "print(type(final_counts))\n",
    "print(final_counts.shape)\n",
    "print(final_counts[0])\n",
    "print(final_counts.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words using Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names  ['and', 'and important', 'and sleeping', 'and technology', 'are', 'are strongly', 'be', 'eating', 'eating and', 'enables', 'enables humans', 'humans', 'humans to', 'important', 'important things', 'in', 'in life', 'is', 'is more', 'is what', 'it', 'it to', 'just', 'just eating', 'life', 'life is', 'life than', 'make', 'make it', 'more', 'more to', 'related', 'related things', 'science', 'science and', 'simplify', 'simplify life', 'sleeping', 'strongly', 'strongly related', 'technology', 'technology are', 'technology enables', 'than', 'than just', 'there', 'there is', 'things', 'things and', 'things too', 'to', 'to be', 'to life', 'to simplify', 'too', 'too in', 'what', 'what you', 'you', 'you make']\n",
      "==================================================\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(4, 60)\n",
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 47)\t2\n",
      "  (0, 48)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 55)\t1\n",
      "[[2 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
      "  0 0 1 1 1 1 0 0 0 0 0 2 1 1 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "count_vect.fit(text_array)\n",
    "print(\"some feature names \", count_vect.get_feature_names())\n",
    "print('='*50)\n",
    "final_counts = count_vect.transform(text_array)\n",
    "print(type(final_counts))\n",
    "print(final_counts.shape)\n",
    "print(final_counts[0])\n",
    "print(final_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF using Bi_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['and', 'and important', 'and sleeping', 'and technology', 'are', 'are strongly', 'be', 'eating', 'eating and', 'enables', 'enables humans', 'humans', 'humans to', 'important', 'important things', 'in', 'in life', 'is', 'is more', 'is what', 'it', 'it to', 'just', 'just eating', 'life', 'life is', 'life than', 'make', 'make it', 'more', 'more to', 'related', 'related things', 'science', 'science and', 'simplify', 'simplify life', 'sleeping', 'strongly', 'strongly related', 'technology', 'technology are', 'technology enables', 'than', 'than just', 'there', 'there is', 'things', 'things and', 'things too', 'to', 'to be', 'to life', 'to simplify', 'too', 'too in', 'what', 'what you', 'you', 'you make']\n",
      "(0, 'and')\n",
      "(1, 'and important')\n",
      "(2, 'and sleeping')\n",
      "(3, 'and technology')\n",
      "(4, 'are')\n",
      "(5, 'are strongly')\n",
      "(6, 'be')\n",
      "(7, 'eating')\n",
      "(8, 'eating and')\n",
      "(9, 'enables')\n",
      "(10, 'enables humans')\n",
      "(11, 'humans')\n",
      "(12, 'humans to')\n",
      "(13, 'important')\n",
      "(14, 'important things')\n",
      "(15, 'in')\n",
      "(16, 'in life')\n",
      "(17, 'is')\n",
      "(18, 'is more')\n",
      "(19, 'is what')\n",
      "(20, 'it')\n",
      "(21, 'it to')\n",
      "(22, 'just')\n",
      "(23, 'just eating')\n",
      "(24, 'life')\n",
      "(25, 'life is')\n",
      "(26, 'life than')\n",
      "(27, 'make')\n",
      "(28, 'make it')\n",
      "(29, 'more')\n",
      "(30, 'more to')\n",
      "(31, 'related')\n",
      "(32, 'related things')\n",
      "(33, 'science')\n",
      "(34, 'science and')\n",
      "(35, 'simplify')\n",
      "(36, 'simplify life')\n",
      "(37, 'sleeping')\n",
      "(38, 'strongly')\n",
      "(39, 'strongly related')\n",
      "(40, 'technology')\n",
      "(41, 'technology are')\n",
      "(42, 'technology enables')\n",
      "(43, 'than')\n",
      "(44, 'than just')\n",
      "(45, 'there')\n",
      "(46, 'there is')\n",
      "(47, 'things')\n",
      "(48, 'things and')\n",
      "(49, 'things too')\n",
      "(50, 'to')\n",
      "(51, 'to be')\n",
      "(52, 'to life')\n",
      "(53, 'to simplify')\n",
      "(54, 'too')\n",
      "(55, 'too in')\n",
      "(56, 'what')\n",
      "(57, 'what you')\n",
      "(58, 'you')\n",
      "(59, 'you make')\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (4, 60)\n",
      "  (0, 59)\t0.27419062636069047\n",
      "  (0, 58)\t0.27419062636069047\n",
      "  (0, 57)\t0.27419062636069047\n",
      "  (0, 56)\t0.27419062636069047\n",
      "  (0, 51)\t0.27419062636069047\n",
      "  (0, 50)\t0.17501232505355918\n",
      "  (0, 28)\t0.27419062636069047\n",
      "  (0, 27)\t0.27419062636069047\n",
      "  (0, 25)\t0.27419062636069047\n",
      "  (0, 24)\t0.14308404345958964\n",
      "  (0, 21)\t0.27419062636069047\n",
      "  (0, 20)\t0.27419062636069047\n",
      "  (0, 19)\t0.27419062636069047\n",
      "  (0, 17)\t0.21617503921079465\n",
      "  (0, 6)\t0.27419062636069047\n",
      "[[0.30700343 0.19469746 0.         0.19469746 0.19469746 0.19469746\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.19469746 0.19469746 0.19469746 0.19469746 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.10160121 0.         0.         0.         0.         0.\n",
      "  0.         0.19469746 0.19469746 0.19469746 0.19469746 0.\n",
      "  0.         0.         0.19469746 0.19469746 0.15350171 0.19469746\n",
      "  0.         0.         0.         0.         0.         0.38939492\n",
      "  0.19469746 0.19469746 0.         0.         0.         0.\n",
      "  0.19469746 0.19469746 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.32788962 0.32788962 0.32788962\n",
      "  0.32788962 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17110641 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32788962\n",
      "  0.32788962 0.         0.         0.         0.25851194 0.\n",
      "  0.32788962 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.2092877  0.         0.         0.32788962\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.19165289 0.         0.24308739 0.         0.         0.\n",
      "  0.         0.24308739 0.24308739 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19165289\n",
      "  0.24308739 0.         0.         0.         0.24308739 0.24308739\n",
      "  0.12685308 0.         0.24308739 0.         0.         0.24308739\n",
      "  0.24308739 0.         0.         0.         0.         0.\n",
      "  0.         0.24308739 0.         0.         0.         0.\n",
      "  0.         0.24308739 0.24308739 0.24308739 0.24308739 0.\n",
      "  0.         0.         0.15515953 0.         0.24308739 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.27419063 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21617504\n",
      "  0.         0.27419063 0.27419063 0.27419063 0.         0.\n",
      "  0.14308404 0.27419063 0.         0.27419063 0.27419063 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.17501233 0.27419063 0.         0.\n",
      "  0.         0.         0.27419063 0.27419063 0.27419063 0.27419063]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf_vect.fit(text_array)\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names())\n",
    "\n",
    "index_list= []\n",
    "for i in range(len(tf_idf_vect.get_feature_names())):\n",
    "    index_list.append(i)\n",
    "zipped_words = zip(index_list,tf_idf_vect.get_feature_names())\n",
    "\n",
    "for i in zipped_words:\n",
    "    print(i)\n",
    "    \n",
    "print('='*50)\n",
    "\n",
    "final_tf_idf = tf_idf_vect.transform(text_array)\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print(final_tf_idf[3])\n",
    "print(final_tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 64)\n",
      "['science and technology are strongly related things and important things too in life', 'Technology enables humans to simplify life', 'there is more to life than just eating and sleeping', 'life is what you make it to be', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life', 'science and technology are important for life']\n",
      "[[0.05831701]]\n"
     ]
    }
   ],
   "source": [
    "test_doc = 'science and technology are important for life'\n",
    "text_array.append(test_doc)\n",
    "tf_idf_vect2 = TfidfVectorizer(ngram_range=(1,2))\n",
    "test_tf_idf = tf_idf_vect2.fit_transform(text_array)\n",
    "print(test_tf_idf.shape)\n",
    "print(text_array)\n",
    "print(cosine_similarity(test_tf_idf[4],test_tf_idf[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science and technology are strongly related things and important things too in life', 'Technology enables humans to simplify life', 'there is more to life than just eating and sleeping', 'life is what you make it to be']\n"
     ]
    }
   ],
   "source": [
    "text_array = []\n",
    "string_1 = 'science and technology are strongly related things and important things too in life'\n",
    "string_2 = 'Technology enables humans to simplify life'\n",
    "string_3 = 'there is more to life than just eating and sleeping'\n",
    "string_4 = 'life is what you make it to be'\n",
    "text_array.append(string_1)\n",
    "text_array.append(string_2)\n",
    "text_array.append(string_3)\n",
    "text_array.append(string_4)\n",
    "print(text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 27)\n",
      "[[2 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 2 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#CV\n",
    "count_vect_new = CountVectorizer()\n",
    "cv_fit = count_vect_new.fit_transform(text_array).todense()\n",
    "print(cv_fit.shape)\n",
    "print(cv_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 27)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[2 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 2 0 1 0 0]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "test_doc = 'I am a disco dancer ta ta ta da'\n",
    "x = count_vect_new.transform([test_doc]).todense()\n",
    "print(x.shape)\n",
    "print(x[0])\n",
    "print(cv_fit[0])\n",
    "print(cosine_similarity(x[0],cv_fit[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
